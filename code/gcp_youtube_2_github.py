# -*- coding: utf-8 -*-
"""gcp_youtube_2_github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VSY6KhVUraSC83ydYutKFY-5S35SOSi5
"""

import base64
from googleapiclient.discovery import build
from bs4 import BeautifulSoup
from langdetect import detect
from datetime import datetime
from  github import Github
import pandas as pd
import requests
import json
import requests


def hello_pubsub(event, context):
    """Triggered from a message on a Cloud Pub/Sub topic.
    Args:
         event (dict): Event payload.
         context (google.cloud.functions.Context): Metadata for the event.

    this funtions pushes the top trending videos data from YouTube to GitHub     
    """

    time = str(datetime.now().isoformat())

    #ensures that cloud function only runs at a certain time
    if time[14:16] == '37':

      youTubeApiKey='youtube_api_key_here'
      youtube=build('youtube','v3',developerKey=youTubeApiKey)

       #getting global trending list fromKwords 
      result = requests.get(f'https://kworb.net/youtube/trending_overall.html')
      src = result.content
      soup = BeautifulSoup(src, 'lxml')

      trending_vids = [i.text for i in soup.find_all('a')]

      eng_trending_vids = []

      #filtering for only english videos with only ascii characters
      for idx, i in enumerate (trending_vids[20:300]):
        try:
          i.encode(encoding='utf-8').decode('ascii')
          if detect(i.lower()) == 'en':
            eng_trending_vids.append(int(idx)+20)
        except:
          continue

      video_ids = []

      #getting the video IDS of the trending videos
      for i in eng_trending_vids:
        try:
          video_id = (str(soup.find_all('a')[i]).split('.html')[0].split('/')[-1])
          video_ids.append(video_id)
        except:
          continue

      data = []

      #getting the data of the videoIDS using the YouTube APi
      for video_id in video_ids:

        if len(data) < 50:
          #making calls until reached the API limit
          try:
            snippets = youtube.search().list(part="snippet", q=video_id).execute()
            username = snippets['items'][0]['snippet']['channelTitle']
            video_title = snippets['items'][0]['snippet']['title']
            publish_time = snippets['items'][0]['snippet']['publishTime']
            r = requests.get(f'https://www.googleapis.com/youtube/v3/videos?part=statistics&id={video_id}&key={youTubeApiKey}')
            comment_count = json.loads(r.text)['items'][0]['statistics']['commentCount']
            dislike_count = json.loads(r.text)['items'][0]['statistics']['dislikeCount']
            like_count = json.loads(r.text)['items'][0]['statistics']['likeCount']
            view_count = json.loads(r.text)['items'][0]['statistics']['viewCount']
            data.append([username, video_title, publish_time, view_count, comment_count, like_count, dislike_count])
          except:
            continue

      pd.options.display.max_colwidth = 10000

      #formatting ingested data as dataframe
      df = pd.DataFrame.from_records(data)
      df.columns = ['username', 'video_title', 'publish_time', 'view_count', 'comment_count', 'like_count', 'dislike count']
      df['api_call_time'] = time

      #using unique api call time to name file
      file_name = f'youtube_{time}.csv'
      g = Github('github_key_here')
      repo = g.get_repo('github_repo_here')
      #parameters are filename, descritption, content
      repo.create_file(file_name, file_name, str(df.to_csv()))


      pubsub_message = base64.b64decode(event['data']).decode('utf-8')
      print(pubsub_message)